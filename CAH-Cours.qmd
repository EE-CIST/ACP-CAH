---
title: "Introduction à la classification"
author: "Claude Grasland (à partir de Daniel J. Denis, 2020)"
format: html
embed-resources: true
bibliography: references.bib
---

```{r, echo=FALSE, message =F, warning=F,error=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(knitr,quietly = T,warn.conflicts = T)
library(dplyr,quietly = T,warn.conflicts = T)
library(reshape2,quietly = T,warn.conflicts = T)
library(ggplot2,quietly = T,warn.conflicts = T)
library(ggrepel)
library(kableExtra)
library(FactoMineR)
```

## OBJECTIFS D'APPRENTISSAGE

-   Comprendre la nature générale d'une procédure de classification non supervisée
-   Construire une matrice de dissimilarité à partir d'un tableau à 1, 2 ou k dimension
-   Construire une classification ascendante hiérarchique à l'aide du critère de Ward
-    Comprendre le lien entre ACP et CAH



## Introduction

La classification consiste d'une manière générale à regrouper dans une même classe des individus qui se ressemblent et à séparer dans des classes différentes ceux qui sont différents. Ce problème très général est au coeur même de toute démarche scientifique et il soulève des questions épistémologiques et philosophiques fondamentales qui dépassent le cadre de cet enseignement. Nous nous limiterons ici à poser le problème de la classification dans le cadre de procédures statistiques appliquées à des variables de type quantitatif continu. Nous montrerons que le problème posé est alors celui de la recherche d'une **classification non supervisée** c'est-à-dire la découverte de ressemblances entre des individus en fonction de critères objectivement reproductibles.


## Classification dans un espace à une dimension

Considérons à titre de premier exemple la consommation moyenne d'alcool (mesurée en kCal/pers/j) de 9 régions d'Europe et d'Afrique


```{r}
don<-readRDS("fao/exemple1.RDS")[,c(1,2)]
rownames(don)<-1:9
kbl(don,caption = "Exemple 1 : variable unique ", row.names = T)

```

Essayons de répondre à des questions d'abord à des questions simples comme :

- **Q1** : quelles sont les deux régions les plus dissemblantes ?
- **Q2** :l’Afrique Occidentale ressemble-t-elle plus à l’Afrique septentrionale ou à l’Afrique Australe ?

Puis à des questions plus complexes comme : 

- **Q3** : Quelle est la meilleure partition en deux classes ?
- **Q4** : Quelle est la meilleure partition en k classes ?

- La question **Q1** est la plus simple et sa réponse ne devrait pas susciter de débat. sachant que la valeur miniumum est de 4 et la valeur maximale de 176, on peut conclure que la plus grande différence est observée entre l'Afrique septentrionale (point n°5) et l'Europe occidentale (point n°7). On peut visualiser leuer éloignement à l'aide d'une figure :

```{r, fig.height=1, fig.width =7}
don$pos<-0
don$code<-1:9
ggplot(don,aes(x=Alcool,y=pos,label = code))+
  geom_point(col="red") +
  geom_text_repel() +
  theme_light()+
  theme(axis.text.y=element_blank(),
        axis.title.y = element_blank(),
        axis.ticks.y=element_blank())

```

- La question **Q2** est en revanche moins simple qu'il n'y paraît car elle peut appeler des réponses différentes selon que l'on décide d'utiliser des différences *absolues* ou des différences *relatives* entre les régions. 

### Distance absolue

Si l'on raisonne en valeur absolue, nous allons construire une matrice de dissimilarité $D_{abs}$ définie par :

$D_{abs}(i,j) = \lvert{X_i-X_j}\rvert$

```{r}
Dabs<-as.matrix(dist(don$Alcool,upper = T))
row.names(Dabs)<-paste0(don$code,". ",don$region)
colnames(Dabs)<-don$code
kable(Dabs, caption = "Matrice des différences absolues")
```
On serait alors tenté de dire que l'Afrique occidentale ressemble plus à l'Afrique septentrionale qu'à l'Afrique Australe puisque les distances observées sont de 17 dans le premier cas et de 80 dans le second.

### Distance relative

Mais on pourrait aussi considérer la distance relative en effectuant pour chaque paire de valeur le rapport entre le maximum et le maximum. Soit la matrice de distance relative $D_{rel}$ définie par :

$D_{rel}(i,j) = \frac{max(X_i,X_j)}{min(X_i,X_j)}$


```{r}
x<-don$Alcool
m<-matrix(nrow = 9,ncol=9)
for (i in 1:9){
  for (j in 1:9){
    m[i,j]= max(x[i],x[j])/min(x[i],x[j])
  }
}
Drel<-m
row.names(Drel)<-paste0(don$code,". ",don$region)
colnames(Drel)<-don$code
kable(Drel, digits=2, caption = "Matrice des différences relatives")
```
On aboutit désormais à une conclusion inverse. En effet le rapport de consommation d'alcool est de 1 à 4.81 dans le cas de l'Afrique australe et de 1 à 5.25 dans le cas del'Afrique septentrionale. 

### Distance logarithmique

On aurait pu aboutir à la même conclusion en calculant les différences absolues entre les logarithmes des valeurs respectives de Xi et Xj soit la matrice $D_{log}$ :

$D_{log}(i,j) = \lvert{log(X_i)-log(X_j)}\rvert$

```{r}
Dlog<-as.matrix(dist(log(don$Alcool),upper = T))
row.names(Dlog)<-paste0(don$code,". ",don$region)
colnames(Dlog)<-don$code
kable(Dlog, caption = "Matrice des différences logarithmiques", digits=2)
```

Ce résultat est logique si on se rappelle que :

$log(\frac{X_i}{X_j}) = log(X_i) - log(X_j)$

Les valeurs affichées dans cette troisième matrice ne sont donc rien d'autre que les logarithmes des valeurs de la seconde matrice.

### Distance euclidienne (au carré)

Présentons pour finir une quatrième matrice de distance correspondant au carré des différences entre les valeurs que nous nommerons distance euclidienne au carré :

$D_{euc}^2(i,j) = (X_i-X_j)^2$

```{r}
Deuc <- Dabs*Dabs
kable(Deuc, caption = "Matrice des différences euclidiennes au carré", digits=2)
```

A première vue cette quatrième mesure de dissimilarité n'a pas grand intérêt puisqu'elle ne fait que reprendre les distances absolues en renforçant leur effet. La distance entre Afrique occidentale et Afrique australe est désormais de $80^2 = 6400$ tandis que celle entre Afrique occidentale et Afrique septentrionale est de $17^2 = 289$. 

En réalité, cette dernière mesure de distance est l'une des plus utilisée dans les méthodes de classification car elle permet d'établir un lien entre la notion de dissimilarité et la notion de variance. La somme de la matrice des distances euclidiennes au carré est en effet proportionelle à la variance de la variable X puisque :


${var}(X) = \frac{1}{n-1}\sum_{i=1}^n{(X_i-\overline{X})^2} = \frac{1}{2.n.(n-1)}\sum_{i=1}^n\sum_{j=1}^n{(X_i-X_j)^2}$

Ce que l'on peut vérifier facilement en calculant la variance de notre indicateur (4541.111) et en la comparant au total de la matrice des distances euclidiennes au carré (653920). Puis en effectuant le calcul $4541.111 \times 9 \times 8 \times 2 = 653920$

### Partition optimale en deux classes

La recherche d'une partition optimale en deux classes dans un espace à une dimension est relativement simple mais elle impose de se fixer une règle précise de décision, c'est-à-dire un critère de performance à optimiser. D'une manière générale, ce critère devra répondre à la défintion proposée en introduction à savoir :

- regrouper les unités qui se ressemblent le plus entre elles
- séparer les unités qui sont les plus différentes entre elles.

Au vu de la distribution de notre variable, il semble assez évident que nous allons regrouper ensemble les quatres régions d'Europe (n°6,7,8,9) à forte consommation d'alcool et les quatre régions d'Afrique (n°2,3,4,5) à faible consommation. Mais on peut hésiter sur l'affectation de la région n°1 qui se situe à peu près à mi-chemin entre les deux groupes. Faut-il couper en A (trait rouge) ou en B (trait bleu) ?

```{r, fig.height=1, fig.width =7}
don$pos<-0
don$code<-1:9
ggplot(don,aes(x=Alcool,y=pos,label = code))+
  geom_point(col="gray50") +
  geom_text_repel() +
  theme_light()+
  geom_vline(aes(xintercept=70), col="red",show.legend = T)+
  geom_vline(aes(xintercept=125), col="blue")+  
  theme(axis.text.y=element_blank(),
        axis.title.y = element_blank(),
        axis.ticks.y=element_blank())


```

Une manière statistique de trancher entre les deux solutions consiste à utiliser l'**analyse de variance** et de tester la part de variance expliquée par un modèle rattachant le point central soit à l'Europe (on coupe en A), soit à l'Afrique (on coupe en B). On construit donc le tableau suivant :

```{r}
don<-don[,1:2]
don$Classes_2A<-c("CL2","CL1","CL1","CL1","CL1","CL2","CL2","CL2","CL2")
don$Classes_2B<-c("CL1","CL1","CL1","CL1","CL1","CL2","CL2","CL2","CL2")
kbl(don)
```

```{r}
modA<- lm(don$Alcool~don$Classes_2A)
summary(modA)
anova(modA)
modB<- lm(don$Alcool~don$Classes_2B)
summary(modB)
anova(modB)
#library(stargazer)
#stargazer(modA,modB, type = "html")
```

L'analyse des résultats montre que la solution A est la meilleure dans la mesure où elle a boutit à  89.4% de variance expliquée (donc interclasse) et 10.6% de variance résiduelle (donc intraclasse). La solution B n'arrive qu'à 83.4% de variance interclasse contre 16.6% de variance intraclasse. 

Il semble donc plus intéressant de regrouper l'Afrique australe avec les pays européens si le critère à optimiser est la variance c'est-à-dire la somme des distances euclidiennes élevées au carré. Les conclusion auraient évidemment pu être différentes si nous avions adopté un autre critère. 

### Partition optimale en k-classes

Supposons maintenant que nous cherchions à diviser notre variable en quatre classes, quelle serait la solution optimale en conservant le critère précédent de minimisation de la variance intra-classe et de maximisation de la variance inter-classe ? 

Le problème posé est d'une grande complexité mathématique lorsqu'il s'applique à de grand tableaux de données. On utilise le plus souvent des algorithmes comme celui de Jenks pour trouver la meilleure solution possible. Parmi les méthodes facilement accessibles dans R-base pour des tableaux de petite taille, ont peut souligner l'intérêt de la méthode des noyaux mobiles  qui consiste à tirer au hasard plusieurs centres de classes et à regrouper autour d'eux les éléments les plus proches jusqu'à atteindre une convergence. En répétant les tirages à sort, on peut espérer se rapprocher de la solution optimale. 

Dans notre exemple, on active la procédure k-means pour 100 tirages au sort :


```{r}
don<-don[,1:4]
class4<-kmeans(don$Alcool,4, nstart=20)
don$Classes_4<-paste0("CL",class4$cluster)
don<-don[order(don$Alcool),]
kbl(don)
```

La solution trouvée par l'algorithme consiste à séparer la région d'Afrique Australe de l'Europe pour en faire une classe à elle toute seule. Puis à diviser les 4 régions d'Afrique en deux paires. 

## Classification dans un espace à 2 dimensions








## Bibliographie
