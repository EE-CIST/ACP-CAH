---
title: "Introduction à l'ACP"
author: "Claude Grasland (à partir de Daniel J. Denis, 2020)"
format: html
embed-resources: true
bibliography: references.bib
---

```{r, echo=FALSE, message =F, warning=F,error=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(knitr,quietly = T,warn.conflicts = T)
library(dplyr,quietly = T,warn.conflicts = T)
library(reshape2,quietly = T,warn.conflicts = T)
library(ggplot2,quietly = T,warn.conflicts = T)
library(kableExtra)
```

## OBJECTIFS D'APPRENTISSAGE

-   Comprendre la nature de l'analyse en composantes principales et son utilité pratique
-   savoir exécuter et interpréter une analyse de composants dans R en utilisant les fonctions R-base *prcomp()* et *princomp()*.
-   savoir tracer des matrices de corrélation entre les variables pour évaluer l'ampleur de leur redondance.
-   Savoir donner un sens concret aux facteurs dérivés de l'ACP.

## Introduction

-   **N.B. Cette introduction est directement adaptée du chapitre 10 de @denis2020, pp. **

L'**analyse en composantes principales**, ou « ACP » en abrégé, est une technique de réduction des données utilisée pour transformer la variance d'un ensemble de *p* variables en moins de *p* dimensions. L'ACP traditionnelle suppose que les variables sont de nature quantitative continue.

Par exemple, supposons qu'un chercheur ait 100 variables dans son ensemble de données. Dans ces variables il y a une certaine quantité de variance. En d'autres termes, l'ensemble complet de variables contient une quantité particulière de variabilité qui constitue l'**information** contenue dans ces variables (s'il n'y avait pas de variabilité, il n'y aurait pas d'information !). Si nous voulions connaître la variance totale des variables, nous pourrions les additionner et obtenir une mesure de la variance totale.

Ce que l'ACP cherche à faire, c'est de prendre cet ensemble original de variables dans *p* dimensions et d'effectuer une transformation sur ces variables de telle sorte que la quantité totale de variance d'origine soit préservée. Par exemple, si nous avons 100 variables représentant une variance totale de 500, la transformation opérée par l'ACP ne modifiera pas cette quantité totale de variance. Ce qu'elle va faire, c'est tenter d'expliquer cette variance avec le moins de nouvelles «composantes» possible, de sorte que les premières composantes extraites expliqueront la majeure partie de la variance dans l'ensemble initial de variables. C'est du moins l'espoir, si la technique réussit...

L'objectif est donc de réussir à résumer la majeure partie de la variance à l'aide des premières composantes (aussi appelées *facteurs*) et donc de "**réduire les données**" à ces quelques facteurs. Au lieu d'avoir besoin d'étudier toutes les variables initiales, le chercheur pourra alors se concentrer sur l'analyse des premiers facteurs qui expliquent la majeure partie de la variance des variables. Considérez à titre d'exemple la figure tirée de @denis2021.

```{r,fig.width=6}
include_graphics("img/Taylor001.jpg")
```

Dans le graphique, les axes d'origine sont A1 et A2. Les "nouveaux" axes après la transformation sont E1 et E2. Bien qu'il existe plusieurs façons d'interpréter le fonctionnement de l'ACP, une méthode courante consiste à visualiser les composants de l'échantillon comme générés en faisant **pivoter les axes de coordonnées** afin qu'ils traversent la dispersion dans le tracé de la variance maximale (@johnson2007).

Par conséquent, nous pouvons voir que l'ACP a effectué une rotation telle que les nouveaux axes optimisent certaines fonctions des données. Quelle fonction est maximisée ? Comme nous le verrons, la rotation se produit de telle sorte que la première composante représente autant de variance que possible dans les variables d'origine, tandis que la seconde composante rend compte autant que possible de la variance des variables d'origine, **mais sous réserve du fait qu'elle est orthogonale à la première composante**. Remarquez que sur la figure, les axes sont perpendiculaires les uns aux autres, ce qui revient à dire qu'ils sont à 90°. C'est la condition d'orthogonalité.

## PRINCIPES DE L'ACP 

-   **N.B. Cette section est directement adaptée du chapitre 10 de @denis2020 mais en modifiant l'exemple retenu par l'auteur**

L'ACP est généralement appliquée à des ensembles de données qui contiennent de nombreuses variables, parfois des centaines, puisque c'est surtout pour ces ensembles de données que l'on souhaite réduire la dimensionnalité. Par exemple, si notre ensemble de données contient 1000 variables, l'ACP peut être utile pour réduire la dimensionnalité à 2 ou 3 dimensions (si la procédure a réussi). De cette façon, des données comme celle-ci font de l'ACP une approche analytique potentiellement appropriée en raison des nombreuses variables ou dimensions avec lesquelles nous commençons. D'autres fois, nous souhaitons effectuer une ACP sur des ensembles de données avec un plus petit nombre de variables, par exemple, 5-10, pour voir si nous pouvons également réduire ou découvrir la dimensionnalité des données.Quel que soit le nombre de variables avec lesquelles vous commencez, l'ACP peut être une technique appropriée pour vos données.

Cependant, pour comprendre l'ACP à partir des premiers principes, il est très utile et pédagogique de commencer par un exemple très simple ne comportant que deux variables. Pourquoi commencer par un exemple avec seulement deux variables ? Nous le faisons parce que cela permet d'apprécier un peu plus ce que fait l'ACP au lieu de se perdre dans un exemple plus complexe où nous pourrions ne pas apprécier ses mécanismes sous-jacents. Une fois que nous aurons apprécié le fonctionnement de l'ACP sur un petit échantillon , nous serons en mesure de considérer des ensembles de données beaucoup plus complexes permettant d'apprécier toute la puissance de la méthode.

### Les consommations de lait et de vin

Nous considérons un tableau issu des données de la FAO composé de 2 variables décrivant 9 individus décrivant la consommation de lait et de vin dans la ration alimentaire quotidienne des habitants des régions d'Europe et d'Afrique. Précisons que ces valeurs sont des estimations tirées d'enquêtes et d'extrapolations et non pas de mesures directes.

```{r, eval=FALSE}
base<-read.csv("fao/FAOSTAT_data_fr_2-6-2023.csv")
don<-base %>% filter(Produit %in% c("Lait - Excl Beurre","Boissons Alcooliques"), 
                     Zone !="Bénin",
                     Année == "2020") %>%
              select(Zone, Produit,Valeur ) %>%
              dcast(formula = Zone~Produit) 
names(don)<-c("region","Lait","Alcool")
code<-as.factor(don$region)
levels(code) <- c("AFR_Sud","AFR_Centre", "AFR_Ouest","AFR_Est","AFR_Nord","EUR_Sud","EUR_Ouest","EUR_Est","EUR_Nord")
row.names(don)<-code
           
saveRDS(don,"fao/exemple1.RDS")
```





```{r}
don <- readRDS("fao/exemple1.RDS")
kbl(don, caption = "Consommation moyenne de lait et d'alcool en Europe et en Afrique en 2020 (en kCal/pers/jour)")
```

Pour gagner en abstraction, nous désignerons par la suite les variables sous la simple forme $X$ et $Y$ et nous donnerons aux individus qui les composent un simple identifiant numérique correspondant à leur numéro de ligne : $1...9$. Nous obtenons ainsi un tableau que l'on appellera *acp.data* et qui est de type `data.frame`:

```{r, echo=T}
X<-don$Alcool
Y<-don$Lait
acp.data<-data.frame(X,Y)
kable(acp.data, caption = "Données")
```

On peut commencer par le visualiser avec la fonction `plot()`

```{r, echo=TRUE}
plot(acp.data, asp=1)
```

Nos données ayant deux dimensions, X et Y, la question posée par l'ACP est la suivante :

-   **Pouvons nous transformer ce tableau de données à deux dimension (X et Y) en un autre tableau à deux dimensions (F1 et F2) de telle sorte que F1 concentre la plus grande part possible de la variance du tableau initial ?**

L'ACP opère donc une transformation sur les variables, avec l'espoir que la première dimension représentera la majeure partie de la variance totale d'origine. Mais cela ne signifie pas pour autant que l'on réduit - dans un premier temps - le nombre de dimensions. Dans la transformation, l'ACP générera généralement autant de dimensions qu'il y a de variables d'origine dans le tableau de données.

L'ACP se donne simplement pour objectif que la majeure partie de la variance des variables d'origine sera expliquée par la première dimension - dans notre exemple - ou par les deux ou trois premières dans un tableau comportant davantage de variables. Si nous avions un tableau avec 100 variables, nous aurions également voulu les transformer en 100 nouvelles dimensions, dans l'espoir que la majeure partie de la variance des dimensions d'origine (variables) puisse être expliquée par les quelques premières composantes. C'est la nature de l'APC.

En ce sens, l'ACP n'est donc rien de plus qu'**une transformation des axes d'origine vers de nouvelles dimensions**. Les caractéristiques de ces nouvelles dimensions (composantes) et le nombre d'entre elles que nous pouvons choisir de conserver est un autre problème (que nous discuterons sous peu).

Pour effectuer l'ACP, nous avons d'abord besoin de quelque chose pour représenter comment les variables $X$ et $Y$ covarient ensemble. Pour cela, nous allons construire la **matrice de covariance** de $X$ et $Y$. Dans R, nous pouvons construire cette matrice sous la forme d'un objet nommé $A$ à l'aide de la fonction `cov()` appliquée à l'ensemble du tableau de données.

```{r, echo=TRUE}
A <- cov(acp.data)
A
```

Rappelons la nature d'une matrice de covariance : Ellel contient les variances des variables le long de la diagonale principale allant du haut à gauche vers le bas droite. Pour nos données, la variance de $X$ est égale à 26455.778 et la variance de $Y$ est égale à 4541.111  . Les covariances entre $X$ et $Y$ sont données par les cases situées en dehors de la diagonale de la matrice. Nous pouvons voir dans la matrice que la covariance entre $X$ et $Y$ est égale à 9602.306. Notez que ces nombres sont les mêmes dans la matrice, car il n'y a qu'une seule covariance entre $X$ et $Y$. Autrement dit, le nombre en bas à gauche de la matrice est le même que le nombre en haut à droite. La matrice de covariance est un exemple de **matrice symétrique**, ce qui signifie que la partie triangulaire supérieure de celle-ci est la même que la partie triangulaire inférieure. La raison pour laquelle nous avons construit une matrice de covariance de $X$ et $Y$ est que dans un instant nous demanderons à R d'effectuer une analyse en composantes directement sur cette matrice.

Pour la démonstration, et pour confirmer que nous avons créé notre matrice de covariance correctement (ainsi que R l'a calculée correctement), nous calculons les variances et la covariance de $X$ et $Y$ manuellement dans R. Tout d'abord, nous confirmons que les variances ont été calculées correctement en utilisant la fonction `var()`de R :

```{r, echo=TRUE}
var(X)
var(Y)
```

On voit que les variances calculées correspondent à celles de la matrice de covariance. Nous aurions également pu utiliser la fonction `diag()` sur la matrice $A$ pour obtenir les valeurs le long de la diagonale principale, qui correspondent bien sûr aux variances ci-dessus :

```{r,echo=TRUE}
diag(A)
```

Nous pouvons enfin confirmer que les valeurs situées hors de la diagonale correspondent bien à la covariance de X et de Y :

```{r, echo=TRUE}
cov(X,Y)
```

### Réalisation de l'ACP

Après avoir produit notre matrice de covariance, nous sommes maintenant prêts à exécuter l'ACP dans R. Nous allons d'abord utiliser la fonction *princomp()* de R pour obtenir les résultats de l'ACP, puis nous montrerons plus tard une autre fonction dans R pour obtenir l'ACP directement à partir du tableau de données

```{r, echo = T}
acp<-princomp(covmat=A)
summary(acp)
```

Que nous apprend ce tableau d'un point de vue statistique ?

-   l'écart-type de la composante 1 est de 173.4 ce qui signifie que sa variance (le carré de l'écart-type) est égale à 30067. Quand à la composante 2 son écart-type est égal à 30.48 ce qui signifie que sa variance est égale à 929. Si on somme ces deux variances on trouve une variance totale de 30996 qui est égale la somme des variances de X et de Y. La transformation opérée par **l'ACP a donc conservée la variance totale (on dit aussi l'inertie) du tableau de départ**.
-   Mais la variance a été redistribuée de façon à être désormais concentrée au maximum sur la première composante. Celle-ci représente en effet 30067/30996 = 97% de l'inertie totale, tandis que la second composante ne représente que 3% de l'inertie totale. Dans le tableau initial la variable X représentait 85.3% de l'inertie et la variable Y  14.7% de l'inertie. L'ACP a donc permis de mettre à jour une dimension fondamentale (composante 1) et une dimension résiduelle (composante 2)

### Coefficients de l'ACP

Pour bien comprendre la nature des deux nouvelles variables, il faut analyser la façon dont elles ont recomposé les variances initiales en opérant une transformation. Pour cela, nous allons demander à R d'afficher les *coefficients* de transformation des variables appelés en anglais *loadings* :

```{r}
acp$loadings
```

Que signifient ces résultats ?

-   la Composante 1 affiche un coefficient de 0.936 pour X et de 0.352 pour Y. Elle opère donc une transformation linéaire de la forme

$F_1 = 0.936(X) + 0.352(Y)$

-   la Composante 2 affiche un coefficient de 0.352 pour X et de -0.936 pour Y. Elle opère donc une transformation linéaire de la forme

$F_2 = 0.352(X) - 0.936(Y)$

-   Les valeurs *SSloadings* pour chaque composante font référence aux sommes des poids au carré des coefficients pour chaque colonne de chargements. Une des contraintes de l'ACP est que la somme des carrés des coefficients de chaque composante soit égale à 1.0. Il s'agit d'une contrainte mathématique permettant d'assurer la conservation des variances des composantes par rapport à la variance totale du tableau initial. On peut vérifier que c'est bien le cas en faisant la somme des carrés des coefficients de chacune des colonnes.

```{r, echo=TRUE}
(0.936)**2 + (0.352)**2
(0.352)**2 + (-0.936)**2

```

### Détails mathématiques sur le calcul des coefficients

Vous vous demandez peut-être à ce stade comment sont obtenus les coefficients précédents. Nous savons qu'ils constituent des éléments de chaque composante, mais nous n'avons pas encore discuté de la manière dont ils sont réellement dérivées. Pour comprendre comment ils sont dérivés, nous devons introduire les concepts mathématiques de **valeurs propres** et de **vecteurs propres**.

Il faut tout d'abord rappeler que que pour toute matrice carrée $\boldsymbol{A}$, il est mathématiquement admis qu'un scalaire $\lambda$ et un vecteur $X$ peuvent être obtenus de sorte que l'égalité suivante soit vraie :

$\boldsymbol{A} X = \lambda X$

Cela signifie que lorsque nous effectuons une transformation sur le vecteur $X$ via la matrice $\boldsymbol{A}$, cela équivaut à transformer le vecteur $X$ en le multipliant par un "scalaire spécial" $\lambda$. Ce scalaire est appelé valeur propre et $X$ est appelé vecteur propre.

En utilisant la décomposition en vecteur propre et valeur propre, l'ACP opère une transformation de la matrice de covariance contenant les variables que nous avons soumises à l'ACP. Il y a encore beaucoup à apprendre sur l'analyse propre, et le lecteur intéressé est encouragé à consulter n'importe quel livre d'algèbre linéaire pour plus de détails, ou un livre complet qui discute de la théorie de l'analyse multivariée de manière plus approfondie, comme Johnson et Wichern (2007) ou Rencher et Christensen (2012). P

Sur le plan mathématique, la plupart des méthodes statistiques multivariées se réduisent, au niveau structurel, à l'analyse des valeurs propres. En R, nous pouvons démontrer que le calcul des valeurs propres et des vecteurs propres que nous avons obtenus dans l'ACP ci-dessus aurait pu l'être par une procédure purement mathématique, la fonction `eigen()`, applicable à n'importe quelle matrice :

```{r, echo=TRUE}
eigen(A)
```

Nous pouvons vérifier que les valeurs propres générées de 30067.8 et 929.04 correspondent exactement à la variance des dimensions obtenues à partir de notre ACP. Nous pouvons également constater que les coefficients de transformation sont les mêmes à un changement de signe prêt (on verra par la suite que la valeur positive ou négative d'un axe est arbitraire).


### Propriétés des composantes principales


Dans notre exemple ci-dessus, nous avons calcule les composantes principales, mais nous n'avons abordé que quelques-unes de leurs propriétés. Nous résumons et développons ici cette discussion. Les composantes obtenues dans une ACP obéissent aux propriétés suivantes :

- **1. La somme des valeurs propres des composantes dérivées de la matrice de covariance correspond à la somme des variances originales des variables brutes**. Pour nos données, rappelons que la somme des variances des valeurs propres est égale à 30996. Ce nombre est le même que celui que nous obtenons en additionnant les variances originales, celles de de X (26455) et de Y  (4541) = 30996. Pourquoi cette propriété est-elle importante ? Elle est essentielle pour comprendre ce que fait l'ACP, car elle révèle que l'ACP ne modifie pas la quantité d'"information" des variables d'origine en termes de variance qu'elles représentent, mais qu'elle transfère simplement cette information (variance) sur de nouvelles dimensions.


- **2. Les composantes successives (qui, rappelons-le, sont des combinaisons linéaires des variables d'origine) sont orthogonales**. Lorsque l'on obtient des composantes principales, la première composante obtenue est celle qui explique le plus possible la variance des variables originales. Il s'agit de la première composante principale. La seconde composante obtenue explique le maximum de variance des variables d'origine, mais avec la contrainte qu'elle soit orthogonale à la première composante. De cette façon, les composantes extraites ne se chevauchent pas.

- **3. La somme des coefficients au carré pour chaque composante est égale à 1.0**. C'est-à-dire que lavariance de chaque composante est maximisée sous réserve de la contrainte que la longueur de chaque composante soit égale à 1.0. Pourquoi cette contrainte est-elle pertinente ? Elle est importante parce que si nous ne soumettons pas les composantes à une telle contrainte, alors en théorie, la variance de la composante donnée pourrait croître sans limite. Autrement dit, la variance de la composante pourrait devenir de plus en plus grande si nous n'avions pas un moyen de la réguler. Le fait de contraindre la somme des coefficients au carré d'être égale à 1.0 est une façon de s'assurer que la variance de chaque composante est "maîtrisée" dans un certain sens. 
- **4. La transformation opérée par l'ACP correspond à un centrage (sur la moyenne des variables) et une rotation des coordonnées **. La position relative des points les uns par rapport aux autres n'est donc pas modifié, ce qui explique que la variance totale soit préservée.


```{r}
plot(X,Y,
     asp=1,
     main = "Centrage et rotation des axes effectuée par l'ACP",
     sub = "F1 en rouge, F2 en bleu")

a1<- (-0.3520806)/(-0.9359697)
b1<- a1*(-mean(X)) + mean(Y)
abline(b1,a1,col="red",lty=2)

arrows(x0 = mean(X),
       y0 = a1*mean(X)+b1,
       x1= mean(X)+100,
       y1 = a1*(mean(X)+100)+b1,
       lwd = 2,
       length=0.1,
       col="red",)
text(300,100, "F1 = 97.0%",col="red", cex=0.8)
 

a2<-(-0.9359697) / 0.3520806
b2<- a2*(-mean(X)) + mean(Y)
abline(b2,a2,col="blue",lty=2)
arrows(x0 = mean(X),
       y0 = a2*mean(X)+b2,
       x1= mean(X)-10,
       y1 = a2*(mean(X)-10)+b2,
       lwd = 2,
       length=0.1,
       col="blue",)

text(180,140, "F2 = 3.0%",col="blue", cex=0.8)

```

### Coordonnées des individus sur les composantes

Une fois définies les composantes par rapport aux variables, on peut s'intéresser aux **individus** qui composent le tableau en calculant leurs coordonnées (en anglais *scores*) dans le nouveau repère. Ces coordonnées sont obtenues en procédant aux opérations de centrage et de réduction en se servant des coefficients des variables obtenus précédemment. La coordonnées $F_i^1$ de l'individu $i$ sur la composante 1 ou la coordonnée  $F_i^2$ de l'individu $i$ sur la composante 2 seront obtenus à l'aide des calculs suivants :


$\begin{cases}F^1_i = a_1(X_i - \bar{X}) + b_1(Y_i - \bar{Y})\\F^2_i = a_2(X_i - \bar{X}) + b_2(Y_i - \bar{Y}) \end{cases}$

On en déduit le tableau ci-dessous qui fournit pour chaque individu (ici chaque pays) ses coordonnées sur chacune des deux composantes :  

```{r}
F1<- (0.9359697)*(X-mean(X)) +  (0.3520806)*(Y-mean(Y))
F2<- (0.3520806)*(X-mean(X)) + (-0.9359697)*(Y-mean(Y))
code<-row.names(don)
tab<-data.frame(code,F1,F2)
rownames(tab)<-1:9
kbl(tab,digits=1,row.names = T, caption = "Coordonnées des individus")
```

On peut également visualiser graphiquement ces nouvelles coordonnées issues du centrage et de la rotation effectuée par l'ACP :

```{r}
plot(F1,F2, 
     asp=1, 
     cex=1, 
     pch=20,
     cex.lab=1,
     cex.axis=1, 
     xlim=c(-300,300),
     xlab = "Comp. F1 : 97% de la variance",
     ylab = "Comp. F2 : 3% de la variance",
     main = "Coordonnées des individus sur les composantes",
     )
text(F1,F2,code,cex=0.8, pos=3,col="red")
abline(v=0,lty=2, lwd=1)
abline(h=0, lty=2, lwd=1)
```


### Interprétation des composantes 

Une fois calculés les *coefficients* des variables et les *coordonnées* des individus, il reste à donner une signification aux composantes. Nous avons en effet transformer les variables initiales en des composantes qui concentrent davantage l'information. Mais cette transformation n'est intéressante que si l'on est capable d'interpréter les résultats. Or, toutes les composantes ne sont pas forcément interprétables et on se limitera le plus souvent à l'interprétation des deux ou trois premières.

Dans notre premier exemple, on devine de façon intuitive qu'il est assez simple d'interpréter les résultats :

-   **la composante 1 exprime le niveau global de consommation de lait ET d'alccol** puisque les variables lait et d'alcool sont positivement corrélées entre elles et avec cette composante. Elle oppose donc les pays à forte consommation de **lait ET d'alcool** et les pays à faible consommation. Elle recoupe visiblement une opposition entre des populations plus riches qui peuvent s'offrir les deux produits (Europe) et des populations plus pauvres (Afrique) qui consomment moins ces produits.

-   **la composante 2 exprime une préférence relative pour le lait OU l'alcool**. Comme elle est **orthogonale** à la première composante, elle va mettre en évidence des différences résiduelles non prises en compte par la composante 1. Elle exprime donc un choix  *toutes choses égales quant au niveau global de consommation de ces deux boissons*. La mention "*toutes choses égales*" est essentielle pour souligner que ces excédents ou déficits sont relatifs au niveau global de consommation des deux boissons.

La position d'un pays sur le graphique (ses coordonnées sur les axes F1 et F2) va donc nous permettre de le caractériser par rapport à nos deux nouveaux indicateurs. Par exemple :

- *l'Europe de l'Ouest* a une coordonnée fortement positive  de +264 sur la composante 1 qui indique une très forte consommation des deux boissons. Sa coordonnée sur la composante 2 est très faible  (+9) ce qui signifie qu'il n'y a pas de préférence particulière pour l'une ou l'autre des deux boissons.
- *l'Afrique de l'Ouest* présente une situation opposée avec une coordonnée de -184 sur la composante 1 qui signale une très faible consommation des deux boissons. Sa coordonnée sur la composante 2 est très faible (+7) ce qui indique, comme en Europe, une absence de préférence pour l'une ou l'autre boisson. 
- *L'Afrique du Nord* affiche une consommation faible des deux boissons (-88) mais relativementplus forte que celle de l'Afrique de l'Ouest. Elle se caractérise par une coordonnée fortement positive sur l'axe 2 (+60) qui indique une préférence pour le lait par rapport à l'alcool, toutes choses égales quant au niveau de consommation. Le facteur religieux explique naturellement dans une large mesure ce résultat. 
- *l'Afrique australe* présente une consommation globale assez voisine de celle de l'Afrique du Nord (-95) au vu de la composante 1. Mais elle montre un comportement opposé sur l'axe 2 (-46) qui signifie une préférence pour l'alcool par rapport au lait, toutes choses égales par ailleurs. Il n'y a pas dans cette région d'interdit religieux de l'alcool et le climat favroise la culture de la vigne et donc la consommation de vin...

### Aides à l'interprétation

Nous avons procédé dans le paragraphe précédent à une interprétation rapide des résultats car les explications étaient relativement simples et évidentes. Mais dans le cas de tableau de données plus important, il est souvent plus délicat de donner un sens aux composantes.  Il est alors utile de mobliser des aides à l'interprétation, notamment en examinant les **contributions des variables et des individus à la constitution des composantes**.

Les aides à l'interprétation sont faciles à obtenir en utilisant le **package FactoMineR**.

```{r}
library(FactoMineR)
row.names(acp.data)<-don$region
acp<-PCA(acp.data, scale.unit = F, graph=F)
```

#### Contribution des variables aux composantes

```{r}

kbl(addmargins(acp$var$contrib,1), caption = "Contribution des variables aux composantes", digits=1)
```

- **Commentaire** : la variable qui contribue le plus à la constitution de la composante 1 est la variable X (87.6%) ce qui est logique puisque c'est elle qui possédait la variance la plus importante. La variable Y contribue inversement pour l'essentiel à la constitution de la composante 2.

#### Contribution des individus aux composantes

On peut proposer une seconde lecture en repérant les individus qui contribuent le plus à la constitution des axes en raison de leurs positions extrêmes par rapport aux variables.

```{r}
kbl(addmargins(acp$ind$contrib,1), caption = "Contribution des individus aux composantes", digits=1)
```

- **Commentaire** : les individus qui contribuent le plus à la constitution de la composante 1 sont les pays à très forte consommation comme l'Europe occidentale (29.0%) et l'Europe septentrionale (15.3%) ou les pays à très faible consommation comme l'Afrique centrale (15.1%)  et l'Afrique occidentale (14%). Ce sont les pays situés aux extrémités opposés de la composante 1. En ce qui concerne la composante 2, elle est clairement déterminée par l'opposition entre l'Afrique septentrionale (49.5%), d'une part, et l'Afrique australe (28.0%) ou l'Europe orientale (12.5%), d'autre part. Comme nous l'avons vu, cela est lié au fait que, toutes choses égales quant à la consommation globale, l'Afrique septentrionale consomme moins d'alcool et plus de lait, tandis que les deux autres régions consomment plus d'alcool et moins de lait.

Il existe d'autres aides à l'interprétation que nous ne détaillerons pas pour l'instant et qui seront vus ultérieurement en travaux dirigés. 

### ACP normé ou non normé ?

Tout au long de ce cours nous avons appliqué la méthode d'**ACP non normée** fondée sur la recherche de valeurs propres et vecteurs propres à partir de la **matrice de covariance**. La particularité de cette méthode est de laisser chaque variable du tableau initial contribuer à proportion de sa variance. Dans notre exemple, la variance de la variable X (consommation d'alcool) était beaucoup plus forte que celle de la variable Y (consommation de lait) de sorte que la constitution des composantes principale a été déterminée en premier lieu par X.

Mais on aurait pu utiliser la méthode plus fréquente de l'**ACP normée** qui se fonde sur la recherche des valeurs propres et des vecteurs propres de la **matrice de corrélation**. Dans ce cas, la variance de chacune des variables du tableau initial est ramenée à 1 et leur moyenne à 0 (variables centrées-réduites), ce qui signifie que chacune d'entre elles contribuera de façon identique à la constitution des composantes. Le résultat sera lors différent comme on peut le voir sur la figure présentant les coefficients et les coordonnées des deux analyses.

```{r, fig.cap= "Comparaison d'ACP normées et non normées", fig.height =6}
rownames(acp.data)<-rownames(don)
acp1<-PCA(acp.data, graph=F,scale.unit = F)
acp2<-PCA(acp.data, graph=F)

par(mfrow=c(2,2))
plot(x=0,
     y=0,
     xlim=c(-150,150),
     ylim=c(-150,150),
     asp=1,
     cex.main=0.8,
     cex.lab=0.6,
     cex.axis=0.6,
     xlab= "Alcool",
     ylab= "Lait",
     main = "ACP non normée / var." )
abline(v=0, lty=2,col="gray50")
abline(h=0, lty=2,col="gray50")
arrows(x0=0, 
       y0=0, 
       length=0.1,
       x1=acp1$var$coord[1,1],
       y1=acp1$var$coord[1,2],
       col="red")
arrows(x0=0, 
       y0=0, 
       length=0.1,
       x1=acp1$var$coord[2,1],
       y1=acp1$var$coord[2,2],
       col="blue")
grid()


plot(x=0,
     y=0,
     xlim=c(-1,1),
     ylim=c(-1,1),
     asp=1,
     cex.main=0.8,
     cex.lab=0.6,
     cex.axis=0.6,
     xlab= "Alcool",
     ylab= "Lait",
     main = "ACP normée / var"  )
abline(v=0, lty=2,col="gray50")
abline(h=0, lty=2,col="gray50")
arrows(x0=0, 
       y0=0, 
       length=0.1,
       x1=acp2$var$coord[1,1],
       y1=acp2$var$coord[1,2],
       col="red")
arrows(x0=0, 
       y0=0, 
       length=0.1,
       x1=acp2$var$cor[2,1],
       y1=acp2$var$cor[2,2],
       col="blue")
grid()



plot(x=acp1$ind$coord[,1],
     y=acp1$ind$coord[,2],
     xlim=c(-250,250),
     ylim=c(-100,100),
     asp=1,
     pch=20,
     cex=0.5,
     cex.main=0.8,
     cex.lab=0.6,
     cex.axis=0.6,
     xlab= "F1",
     ylab= "F2",
     main = "ACP non normée / ind. " )
abline(v=0, lty=2,col="gray50")
abline(h=0, lty=2,col="gray50")
grid()

plot(x=acp2$ind$coord[,1],
     y=acp2$ind$coord[,2],
     xlim=c(-2,2),
     ylim=c(-1,1),
     asp=1,
     pch=20,
     cex=0.5,
     cex.main=0.8,
     cex.lab=0.6,
     cex.axis=0.6,
     xlab= "F1",
     ylab= "F2",
     main = "ACP non normée /ind." )
abline(v=0, lty=2,col="gray50")
abline(h=0, lty=2,col="gray50")
grid()



```

- **Commentaire** : Les résultats des deux analyses sont ici très proches mais on peut noter que la part de variance expliquée par la composante 1 est plus faible dans le cas de l'ACP normée (93.8%) que dans celui de l'ACP non normée (97.0%). On note également que cans le cas de l'ACP normée, les coefficients des variables ont une somme de carré égale à 1, ce qui permet de les représenter à l'intérieur d'un **cercle des corrélations** ou l'angle que fait une variable avec un facteur est d'autant plus faible que la corrélation est forte. On voit donc tout de suite que l'axe 1 est fortement corrélé positivement avec les variables X et Y tandis que l'axe 2 oppose ces deux variables. 

### Quelle méthode choisir ?

D'un point de vue théorique, les deux méthodes peuvent apporter des résultats intéressants et on peut être amené à choisir l'une ou l'autre. Néanmoins nous conseillons au débutant de

- **Toujours choisir l'ACP normée lorsque les unités de mesure des variables sont différentes**. Car il y a de fortes chances que les variance ne soient pas de nature et d'ordre de grandeur comparable si on  mélange des variables telles que la densité de population, le taux de mortalité infantile ou le nombre d'enfants par femme. 

- **Commencer par une ACP normée lorsque les unités de mesure sont comparables** puis confronter dans un deuxième temps les résultats à ceux d'une ACP non normée et examiner les différences.


## EXEMPLE D'APPLICATION

### Données

On reprend les données FAOSTAT utilisées dans l'exemple pédagogique de cours, mais en conservant l'ensemble des variables relatives à la consommation calorique des habitants des régions d'Europe et d'Afrique. Nous procédons toutefois à des regroupements en cinq types principaux d'apports caloriques :

- **Viandes** : abats, viandes, poissons, lait, graisses animales, ...
- **Céréales** : blé, maïs, riz, sorgho...
- **Huiles** : cultures oléagineuses, huiles végétales
- **Sucres** : cultures sucrières, sucres et édulcorants
- **Légumes**: légumes, fruits, ignames, pommes de terres, ...
- **Divers** :  épices, stimulants, boissons alcooliques


```{r, eval = FALSE}
base<-read.csv("fao/FAOSTAT_data_fr_2-6-2023.csv")
base$Produit<-as.factor(base$Produit)
levels(base$Produit)
levels(base$Produit) <-c("Animaux" ,                  
 "Divers"    ,
 "Céréales"   ,
"Huiles"   ,
 "Sucres"      ,
"Divers"                  ,
 "Légumes"             ,
"Légumes"       ,
"Animaux"       ,
"Huiles"        ,
"Animaux"      ,
"Divers"                 ,
"Divers"     ,
"Divers"            ,
"Animaux"                   ,
"Animaux" ,
NA       ,
"Animaux",
NA      ,
"Légumes"            ,
"Divers"              ,
"Sucres"     ,
NA          ,
"Animaux" )

don<-base %>% filter(is.na(Produit)==F, 
                #     Zone !="Bénin",
                     Année == "2020") %>%
              select(Zone, Produit,Valeur ) %>%
              dcast(formula = Zone~Produit,value.var = "Valeur",fun.aggregate = sum) 

#names(don)<-c("Zone","Cereal","Fruit1","Fruit2","Legum1","Legum2","Racine")
code<-as.factor(don$Zone)
levels(code)
levels(code) <- c("AFR_Sud","AFR_Centre", "AFR_Ouest","AFR_Est","AFR_Nord","Bénin","EUR_Sud","EUR_Ouest","EUR_Est","EUR_Nord")
code
row.names(don)<-as.character(code)
don<-don[,c(2,4,7,5,6,3)]  
don$TOTAL<-apply(don,1,sum)
don<-don[c(1:5,7:10,6),]
saveRDS(don, "fao/exemple3.RDS")
don<-don[-10,-7]
saveRDS(don, "fao/exemple2.RDS")
```


```{r}
don2<-readRDS("fao/exemple2.RDS")


kbl(don2, caption = "Consommation alimentaire en kCal/pers/jou (tableau brut)",digits=2)

```

On examine pour chaque variable quelques paramètres principaux afin d'évaluer le rôle potentiel qu'elles vont jouer dans une ACP selon que celle-ci sera normée ou non.

```{r}
m<-apply(don2,2,mean)
e<-apply(don2,2,sd)
cv<-e/m
v<-apply(don2,2,var)
v2<-100*v/sum(v)
tab<-rbind(m,e,cv,v,v2)

row.names(tab)<- c("Moyenne", "écart-type","coeff. de variation ","variance", "variance (%)")
kbl(tab, digits=c(1,1,2,0,1),caption = "Paramètres principaux des variables brutes")
```


Ce tableau montre que certaines variables créent beaucoup plus de différences que d'autres entre les pays, en particulier la consommation de produits animaux qui représenterait 44% de la variance totale si on effectuait une ACP non normée. Les céréales, qui sont pourtant davantage consommée génèrent une variance plus faible (26.7%). Les légumes enfin présentent comme la viande une très forte variabilité (c.v. = 0.7) alors même que leur poids dans le régime alimentaire est plus limitéque celui des céréales.

On décide dans le cas présent d'effectuer une **ACP normée**, ce qui revient à modifier le tableau initial et à le remplacer par des variables centrées et réduites :

```{r}
don2s<-scale(don2)
kbl(don2s, caption = "Consommation alimentaire en kCal/pers/jou (tableau standardisé)",digits=2)
```

Ce nouveau tableau est désormais exprimé en distance à la moyenne mesurée en écart-type. Il permet de repérer facilement les consommations exceptionelles fortes de certains produits dans certaines régions , notamment les céréales en Afrique du Nord (+2.06) ou les légumes en Afrique Centrale (+2.24). En sens inverse on repère des consommations inéférieures à la moyenne telles que les produis animaux dans l'ensemble de l'Afrique, les céréales en Afrique centrale (-1.48), les huiles en Afrique de l'Est (-1.50), etc.  

Du fait de sa transformation, le tableau standardisé offre désormais une contribution égale à chacune des variables puisqu'elles ont toutes une moyenne égale à 0 et un écart-type égal à 1. La part de variance apportée par chacune des six variables sera donc de 1/6 = 16.7%

```{r}
m<-apply(don2s,2,mean)
e<-apply(don2s,2,sd)
v<-apply(don2s,2,var)
v2<-100*v/sum(v)
tab<-rbind(m,e,v,v2)

row.names(tab)<- c("moyenne", "écart-type","variance", "variance (%)")
kbl(tab, digits=1,caption = "Paramètres principaux des variables standardisées")
```
 

### Examen de la matrice des corrélations

Avant de procéder à l'ACP, on examine les corrélations entre les variables à l'aide de la fonction `cor()`

```{r}
matcor<-cor(don2s)
kbl(matcor, digits=2, caption = "Coefficient de corrélation de Pearson")
```

On peut compléter cette matrice par une visualisation de la forme des relations entre l'ensemble des paires de variables à l'aide de la fonction *pairs()* de R-base ou, mieux encore, de la fonction *ggpairs()* du package`GGally`.  Cette dernière ajoute un test de significativité pour chaque corrélation.

```{r,fig.width=8,fig.height=8}
library(GGally)
don2s<-as.data.frame(don2s)
pairs(don2s,)
ggpairs(don2s)

```

### Caclul de l'ACP

On va utiliser le package **FactomineR** pour réaliser l'analyse en composante principales à l'aide de la fonction `PCA()`. Le programme-type ci-dessous permet d'obtenir rapidement l'essentiel des résultats :

.
```{r, echo=TRUE}
library(FactoMineR)
monACP <- PCA(don2s)
summary(monACP)
```

Comme on peut le voir, l'application de la fonction PCA provoque l'apparition de deux graphiques, l'un relatif aux variables et l'autre aux individus.  Quant à la fonction summary, elle génère des tableaux relatifs soit aux variables, soit aux individus. 
`

### Analyse des valeurs propres

L'analyse des valeurs propres permet de savoir quelle est la part de variance prise en compte par chacun des facteurs (on dit aussi *composantes*) de l'analyse. Les facteurs sont orthogonaux c'est-à-dire statistiquement indépendants (corrélation = 0). Le premier facteur est celui qui résume le mieux le nuage de point. Puis le second facteur est celui qui résume le maximum de variance résiduelle, etc. Il y a autant de facteurs que de variables de sorte que les facteurs sont d'autant plus pertinents qu'ils résument une part de la variance supérieure à 1/k ou k est le nombre de variables. Les facteurs qui résument plus qu'une variable ont une valeur propre supérieure à 1 et doivent être interprétés en priorité.

```{r}
valprop<-monACP$eig
kbl(valprop, 
      digits=2,
      caption = "Tableau des valeurs propres",
      col.names = c ("Valeurs propres", "Variance (%)", "Variance cumulée (%)"))
```

- Le premier facteur résumé 60.8% de la variance totale des six variables. Chaque variable comptant pour 1/6e = 16.7%, sa valeur propre est égale à 60.8 / 16.7 = 3.65. Il est donc très significatif et résume à lui seul l'effet de presque quatre variables initiales. 

- Le second facteur résume quant à lui 25.7% de la variance totale, ce qui plus qu'une variable isolée et lui donne donc une valeur propre de 25.7/16.7 = 1.54. Il demeure de ce fait interprétable et, combiné avec le premier facteur il permet de rendre compte de 86.5% de la variance totale c'est-à-dire de l'information 

Les facteurs suivants ont des valeurs propres très faibles (<1) et ne seront donc pas analysés dans la suite puisque notre objectif est de résumer l'information à l'aide d'un nombre d'indicateurs plus faible que le nombre initial de variables. 


### Analyse des corrélations des variables avec les facteurs

Nous avons donc retenu deux facteurs mais il faut maintenant les interpréter en examinant les variables qui en sont responsables. Nous allons pour cela extraire trois autres tableaux mesurant les **corrélations** des variables avec les facteurs ainsi que leurs **contributions** à la constitution de ceux-ci.


```{r}
corvar<-monACP$var$cor[,1:5]
kbl(corvar,digits=2,caption =" Corrélation des variables avec les facteurs")

ctrvar<-monACP$var$contrib[,1:5]
kbl(ctrvar,digits=2,caption =" Contribution des variables aux facteurs")


```

- le premier facteur est très fortement corrélé positivement avec la consommation de produits animaux (+0.94), de sucres (+0.91), d'huiles (+0.83) et de produits divers (+0.82). Il est corrélé négativement avec la consommation de légumes (-0.75) . Il n'est pas corrélé du tout avec la consommation de céréales (+0.01). L'analyse des contribution confirme que ce facteur est déterminé de façon approximativement équivalente par 5 des 6 variables mais pas du tout par la consommation de céréales. Au total, ce premier facteur oppose donc des régions à forte consommation de viandes, huiles ou sucre à des régions consommant prioritairement des légumes. La consommation de céréales n'intervient pas dans cette dimension.  


- le second facteur est  quant à lui précisément par la consommation de céréales  qui est très fortement corrélée négativement (-0.97) et s'oppose à la consommaion de dont la contribution est de 61%.  La consommation de légume se situe à l'opposé de cet axe avec une corrélation de +0.58 et une contribution de 22%. Ce deuxième facteur met donc en évidence une opposition entre les régions dont le régime alimentaire est plutôt tourné vers les légumes ou plutôt tourné vers les céréales, toutes choses égales quant à la consommation générale d'animaux, sucre ou huile qui a été prise en compte par le premier facteur.


On peut maintenant visualiser à nouveau la figure générée automatiquement par FactoMineR afin de comprendre sa signification.

```{r}
plot.PCA(monACP,choix = "varcor")
```

- **Commentaire** : Pour chaque variable, l'angle qu'elle fait avec l'un ou l'autre des deux axes correspond à son niveau de corrélation. Quant à la longueur du vecteur, elle permet de savoir si la variable est convenablement représenté par l'un ou l'autres des deux axes. Ici toutes les variables sont bien représentées et on retrouve grahiquement les résultats obtenus par l'analyse détaillée des tableaux de corrélation et contribution. 


### Analyse des coordonnées des individus sur les facteurs

Nous allons maintenant examiner comment les individus se positionnent sur les facteurs. Comme dans le cas précédent, nous allons extraire deux tableaux, l'un correspondant aux **coordonnées** des individus sur les facteurs, l'autre à leur **contribution** à la constitution des facteurs. 

```{r}

cooind<-monACP$ind$coord
kable(cooind,digits=2,caption =" Coordonnées des individus sur les facteurs")

ctrind<-monACP$ind$contrib
kable(ctrind,digits=2,caption =" Contribution des individus aux facteurs")

```


- le premier axe oppose très clairement les quatre régions d'Europe (coordonnées fortement positives) aux trois régions d'Afrique centrale, orientale et occidentale (coordonnées fortement négatives). L'Afrique septentrionale et l'Afrique australe ont des coordonnées plus proches de zéro et sont donc moins concernées par l'opposition. L'analyse des contributions confirme que l'opposition principale se fait entre l'Afrique centrale (35%), l'Afrique de l'ouest (11%) et l'Afrique de l'ouest (10%), d'une part, et l'Europe de l'ouest (16%), l'Europe du sud (13%) ou l'Europe du nord (10%) d'autre part. Alors que les pays africains se caractérisent par une plus forte consommation de légumes (ignames, manioc, etc.), les pays européens se caractérisent par une plus forte consommation de viandes, huiles, sucres et autres produits divers.  

- le second axe met à jour une opposition secondaire, liée  à la part des céréales dans le régime alimentaire. Elle oppose cette fois-ci nettement l'Afrique du Nord et l'Afrique du Sud à l'Afrique centrale, les deux premières ayant un régime à forte proportion de céréales tandis que les légumes prédominent dans la dernière. Les autres régions ont des contributions plus faibles 

On peut maintenant visualiser à nouveau la figure générée automatiquement par FactoMineR

```{r}
plot.PCA(monACP,choix = "ind")
```

- **Commentaire** : A partir du moment où l'on a bien interprété la signification des axes, il devient plus facile de comprendre ce que signifie la position de chaque pays. Les proximités entre les points permettent par ailleurs de visualiser des proximités entre individus 


### Ajout de variables et d'individus supplémentaires

Il est possible d'ajouter dans une ACP des **variables supplémentaires** et des **individus supplémentaires** qui correspondent respectivement à l'ajout de lignes et de colonnes au tableau initial. Ces variables et individus supplémentaires ne changent pas les composantes de l'ACP puisque leur contribution sera nulle. Mais on peut calculer les corrélations des variables supplémentaires avec les facteurs ainsi que les coordonnées des individus supplémentaires sur les axes. 

```{r}
don3<-readRDS("fao/exemple3.RDS")
 
kbl(don3, caption = "Ajout d'un individu et d'une variable supplémentaires") %>% 
              column_spec(8,italic=T,color = "blue") %>%
              row_spec(10,italic=T,color = "blue")

```


```{r}
monACP2<-PCA(don3,ind.sup = 10,quanti.sup = 7 )
```

- **Interprétaton de la variable supplémentaire TOTAL ** : La variable supplémentaire *TOTAL*, qui représente la consommation totale en kCal/pers/jour est corrélées positivement de façon presque parfaite avec l'axe 1 (+0.93). Elle permet de montrer que l'axe 1 ne décrit pas simplement une *différence qualitative* de régime alimentaire (légumes / viandes, huiles ou sucre) mais plutôt une *différence quantitative* (faible / fort niveau de kCal). 

- **Interprétation de l'individu supplémentaire Bénin** : l'ajout du Bénin au tableau ne modifie pas non plus les composantes factorielles mais il permet de situer sa position par rapport aux autres individus dans le plan factoriel des axes 1 et 2. On voit alors que sa position est très proche de celle de sa région d'appartenance (Afrique de l'Ouest) mais aussi de la région voisine d'Afrique Centrale. Son régime alimentaire est donc caractérisé par une ration alimentaire plutôt faible avec peu de viandes, huile ou sucre (coordonnées négatives sur l'axe 1) mais aussi une quantité relativement plus importante de légumes que de céréales (coordonnées positives sur l'axe 2).

### Utilisation d'une ACP non normée

Comme cela a été expliqué dans la première partie de ce chapitre, on préfère généralement utiliser des ACP normées lorsque l'on utilise des variables à unité de mesure hétérogènes. Mais dans l'exemple présent, toutes les variables sont exprimées dans la même unité et la question est plutôt de savoir quel poids on souhaite donner aux différentes variables. En réalisant une ACP normée, nous avons choisi de donner le même poids aux 5 variables dans la constitution des axes, alors même que nous avons vu que chacune d'elle a une moyenne et une variance différente. Nous aurions donc pu effectuer une ACP non normée en modifiant très légèrement notre programme, ce qui aurait donné un avantage plus grand aux variables à forte variance telles que la consommation de viande ou de céréales.

```{r, echo=TRUE}
monACP3<-PCA(don3,scale.unit = F, ind.sup = 10, quanti.sup = 7)
summary(monACP3)
```
- **Commentaire** : En comparant les résultats de cette ACP non normée avec ceux de l'ACP normée, on voit que les axes factoriels sont désormais surtout déterminés par les trois variables qui avaient la plus forte variance à savoir les consommations de viande, de céréales et de légume. Les trois autres variables (consommations de sucre, huile et divers) jouent désormais un rôle beaucoup plus faible dans la constitution des axes. La position relative des individus n'est plus la même que précédemment ce qui est logique puisque les facteurs 1 et 2 sont de nature différente.


## Bibliographie
