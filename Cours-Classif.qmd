---
title: "COURS : Classification"
author: "Claude Grasland"
format: html
embed-resources: true
bibliography: references.bib
---

```{r, echo=FALSE, message =F, warning=F,error=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(knitr,quietly = T,warn.conflicts = T)
library(dplyr,quietly = T,warn.conflicts = T)
library(reshape2,quietly = T,warn.conflicts = T)
library(ggplot2,quietly = T,warn.conflicts = T)
library(ggrepel)
library(kableExtra)
library(FactoMineR)
library(factoextra)
```

## OBJECTIFS D'APPRENTISSAGE





## Introduction

Les objectifs de ce cours sont :

-   Comprendre la nature générale d'une procédure de classification non supervisée
-   Construire une matrice de dissimilarité à partir d'un tableau à 1, 2 ou k dimension
-   Appliquer la procédure k-means
-   Construire une classification ascendante hiérarchique à l'aide du critère de Ward
-    Comprendre le lien entre ACP et CAH

La classification consiste d'une manière générale à regrouper dans une même classe des individus qui se ressemblent et à séparer dans des classes différentes ceux qui sont différents. Ce problème très général est au coeur même de toute démarche scientifique et il soulève des questions épistémologiques et philosophiques fondamentales qui dépassent le cadre de cet enseignement. Nous nous limiterons ici à poser le problème de la classification dans le cadre de procédures statistiques appliquées à des variables de type quantitatif continu. Nous montrerons que le problème posé est alors celui de la recherche d'une **classification non supervisée** c'est-à-dire la découverte de ressemblances entre des individus en fonction de critères objectivement reproductibles.


## UNE DIMENSION

Considérons à titre de premier exemple la consommation moyenne d'alcool (mesurée en kCal/pers/j) de 9 régions d'Europe et d'Afrique


```{r}
don<-readRDS("fao/exemple1.RDS")[,c(1,2)]
rownames(don)<-1:9
kbl(don,caption = "Exemple 1 : variable unique ", row.names = T)

```

Essayons de répondre à des questions d'abord à des questions simples comme :

- **Q1** : quelles sont les deux régions les plus dissemblantes ?
- **Q2** :l’Afrique Occidentale ressemble-t-elle plus à l’Afrique septentrionale ou à l’Afrique Australe ?

Puis à des questions plus complexes comme : 

- **Q3** : Quelle est la meilleure partition en deux classes ?
- **Q4** : Quelle est la meilleure partition en k classes ?

- La question **Q1** est la plus simple et sa réponse ne devrait pas susciter de débat. sachant que la valeur miniumum est de 4 et la valeur maximale de 176, on peut conclure que la plus grande différence est observée entre l'Afrique septentrionale (point n°5) et l'Europe occidentale (point n°7). On peut visualiser leuer éloignement à l'aide d'une figure :

```{r, fig.height=1, fig.width =7}
don$pos<-0
don$code<-1:9
ggplot(don,aes(x=Alcool,y=pos,label = code))+
  geom_point(col="red") +
  geom_text_repel() +
  theme_light()+
  theme(axis.text.y=element_blank(),
        axis.title.y = element_blank(),
        axis.ticks.y=element_blank())

```

- La question **Q2** est en revanche moins simple qu'il n'y paraît car elle peut appeler des réponses différentes selon que l'on décide d'utiliser des différences *absolues* ou des différences *relatives* entre les régions. 

### Distance absolue

Si l'on raisonne en valeur absolue, nous allons construire une matrice de dissimilarité $D_{abs}$ définie par :

$D_{abs}(i,j) = \lvert{X_i-X_j}\rvert$

```{r}
Dabs<-as.matrix(dist(don$Alcool,upper = T))
row.names(Dabs)<-paste0(don$code,". ",don$region)
colnames(Dabs)<-don$code
kable(Dabs, caption = "Matrice des différences absolues")
```
On serait alors tenté de dire que l'Afrique occidentale ressemble plus à l'Afrique septentrionale qu'à l'Afrique Australe puisque les distances observées sont de 17 dans le premier cas et de 80 dans le second.

### Distance relative

Mais on pourrait aussi considérer la distance relative en effectuant pour chaque paire de valeur le rapport entre le maximum et le maximum. Soit la matrice de distance relative $D_{rel}$ définie par :

$D_{rel}(i,j) = \frac{max(X_i,X_j)}{min(X_i,X_j)}$


```{r}
x<-don$Alcool
m<-matrix(nrow = 9,ncol=9)
for (i in 1:9){
  for (j in 1:9){
    m[i,j]= max(x[i],x[j])/min(x[i],x[j])
  }
}
Drel<-m
row.names(Drel)<-paste0(don$code,". ",don$region)
colnames(Drel)<-don$code
kable(Drel, digits=2, caption = "Matrice des différences relatives")
```
On aboutit désormais à une conclusion inverse. En effet le rapport de consommation d'alcool est de 1 à 4.81 dans le cas de l'Afrique australe et de 1 à 5.25 dans le cas del'Afrique septentrionale. 

### Distance logarithmique

On aurait pu aboutir à la même conclusion en calculant les différences absolues entre les logarithmes des valeurs respectives de Xi et Xj soit la matrice $D_{log}$ :

$D_{log}(i,j) = \lvert{log(X_i)-log(X_j)}\rvert$

```{r}
Dlog<-as.matrix(dist(log(don$Alcool),upper = T))
row.names(Dlog)<-paste0(don$code,". ",don$region)
colnames(Dlog)<-don$code
kable(Dlog, caption = "Matrice des différences logarithmiques", digits=2)
```

Ce résultat est logique si on se rappelle que :

$log(\frac{X_i}{X_j}) = log(X_i) - log(X_j)$

Les valeurs affichées dans cette troisième matrice ne sont donc rien d'autre que les logarithmes des valeurs de la seconde matrice.

### Distance euclidienne (au carré)

Présentons pour finir une quatrième matrice de distance correspondant au carré des différences entre les valeurs que nous nommerons distance euclidienne au carré :

$D_{euc}^2(i,j) = (X_i-X_j)^2$

```{r}
Deuc <- Dabs*Dabs
kable(Deuc, caption = "Matrice des différences euclidiennes au carré", digits=2)
```

A première vue cette quatrième mesure de dissimilarité n'a pas grand intérêt puisqu'elle ne fait que reprendre les distances absolues en renforçant leur effet. La distance entre Afrique occidentale et Afrique australe est désormais de $80^2 = 6400$ tandis que celle entre Afrique occidentale et Afrique septentrionale est de $17^2 = 289$. 

En réalité, cette dernière mesure de distance est l'une des plus utilisée dans les méthodes de classification car elle permet d'établir un lien entre la notion de dissimilarité et la notion de variance. La somme de la matrice des distances euclidiennes au carré est en effet proportionelle à la variance de la variable X puisque :


${var}(X) = \frac{1}{n-1}\sum_{i=1}^n{(X_i-\overline{X})^2} = \frac{1}{2.n.(n-1)}\sum_{i=1}^n\sum_{j=1}^n{(X_i-X_j)^2}$

Ce que l'on peut vérifier facilement en calculant la variance de notre indicateur (4541.111) et en la comparant au total de la matrice des distances euclidiennes au carré (653920). Puis en effectuant le calcul $4541.111 \times 9 \times 8 \times 2 = 653920$

### Partition optimale en deux classes

La recherche d'une partition optimale en deux classes dans un espace à une dimension est relativement simple mais elle impose de se fixer une règle précise de décision, c'est-à-dire un critère de performance à optimiser. D'une manière générale, ce critère devra répondre à la défintion proposée en introduction à savoir :

- regrouper les unités qui se ressemblent le plus entre elles
- séparer les unités qui sont les plus différentes entre elles.

Au vu de la distribution de notre variable, il semble assez évident que nous allons regrouper ensemble les quatres régions d'Europe (n°6,7,8,9) à forte consommation d'alcool et les quatre régions d'Afrique (n°2,3,4,5) à faible consommation. Mais on peut hésiter sur l'affectation de la région n°1 qui se situe à peu près à mi-chemin entre les deux groupes. Faut-il couper en A (trait rouge) ou en B (trait bleu) ?

```{r, fig.height=1, fig.width =7}
don$pos<-0
don$code<-1:9
ggplot(don,aes(x=Alcool,y=pos,label = code))+
  geom_point(col="gray50") +
  geom_text_repel() +
  theme_light()+
  geom_vline(aes(xintercept=70), col="red",show.legend = T)+
  geom_vline(aes(xintercept=125), col="blue")+  
  theme(axis.text.y=element_blank(),
        axis.title.y = element_blank(),
        axis.ticks.y=element_blank())


```

Une manière statistique de trancher entre les deux solutions consiste à utiliser l'**analyse de variance** et de tester la part de variance expliquée par un modèle rattachant le point central soit à l'Europe (on coupe en A), soit à l'Afrique (on coupe en B). On construit donc le tableau suivant :

```{r}
don<-don[,1:2]
don$Classes_2A<-c("CL2","CL1","CL1","CL1","CL1","CL2","CL2","CL2","CL2")
don$Classes_2B<-c("CL1","CL1","CL1","CL1","CL1","CL2","CL2","CL2","CL2")
kbl(don)
```

```{r}
modA<- lm(don$Alcool~don$Classes_2A)
summary(modA)
anova(modA)
modB<- lm(don$Alcool~don$Classes_2B)
summary(modB)
anova(modB)
#library(stargazer)
#stargazer(modA,modB, type = "html")
```

L'analyse des résultats montre que la solution A est la meilleure dans la mesure où elle a boutit à  89.4% de variance expliquée (donc interclasse) et 10.6% de variance résiduelle (donc intraclasse). La solution B n'arrive qu'à 83.4% de variance interclasse contre 16.6% de variance intraclasse. 

Il semble donc plus intéressant de regrouper l'Afrique australe avec les pays européens si le critère à optimiser est la variance c'est-à-dire la somme des distances euclidiennes élevées au carré. Les conclusion auraient évidemment pu être différentes si nous avions adopté un autre critère. 

### Partition optimale en k-classes

Supposons maintenant que nous cherchions à diviser notre variable en quatre classes, quelle serait la solution optimale en conservant le critère précédent de minimisation de la variance intra-classe et de maximisation de la variance inter-classe ? 

Le problème posé est d'une grande complexité mathématique lorsqu'il s'applique à de grand tableaux de données. On utilise le plus souvent des algorithmes comme celui de Jenks pour trouver la meilleure solution possible. Parmi les méthodes facilement accessibles dans R-base pour des tableaux de petite taille, ont peut souligner l'intérêt de la méthode des noyaux mobiles  qui consiste à tirer au hasard plusieurs centres de classes et à regrouper autour d'eux les éléments les plus proches jusqu'à atteindre une convergence. En répétant les tirages à sort, on peut espérer se rapprocher de la solution optimale. 

Dans notre exemple, on active la procédure k-means pour 100 tirages au sort :


```{r}
don<-don[,1:4]
class4<-kmeans(don$Alcool,4, nstart=20)
don$Classes_4<-paste0("CL",class4$cluster)
don<-don[order(don$Alcool),]
kbl(don)
```

La solution trouvée par l'algorithme consiste à séparer la région d'Afrique Australe de l'Europe pour en faire une classe à elle toute seule. Puis à diviser les 4 régions d'Afrique en deux paires. 

## DEUX DIMENSIONS

Examinons maintenant le cas d'une espace à deux dimensions en reprenant l'exemple utilisé dans l'introduction à l'ACP : 

```{r}
don <- readRDS("fao/exemple1.RDS")
tab<-don[,c(1,3,2)]
row.names(tab)<-1:9
kbl(tab, caption = "Consommation moyenne de lait et d'alcool en Europe et en Afrique en 2020 (en kCal/pers/jour)",row.names = T)
```

### Distance euclidienne non normée

Les distances euclidiennes correspondent ici à la distance entre les points dans le plan constitué par nos deux variables ou la consommation de lait est la coordonnée X et la consommation d'Alcool la coordonnée Y : 

```{r}
ggplot(tab, aes(x=Lait,y=Alcool, label=1:9)) + 
       geom_point() +
       ggrepel::geom_label_repel()+
        coord_fixed(ratio = 1)+
  theme_light() +
  ggtitle("Espace à 2 dimensions non normé") 


```


#### Matrice de distance 

L'examen du graphique permet de deviner visuellement quelles unités sont les plus proches les unes des autres. On voit ainsi que le point n°1 semble plus proche du point n°5 que du point n°8, ce que l'on peut confirmer en calculant la distance euclidienne dont on rappelle la formule :

$D_{ij}=\sqrt{(X_i-X_j)^2+(Y_i-Y_j)^2}$

$D_{1,5}=\sqrt{(101-4)^2+(90-134)^2} = \sqrt{9409+1936} = 106.5$

$D_{1,8}=\sqrt{(101-160)^2+(90-290)^2} = = \sqrt{3481+40000} = 208.5$

```{r}
(200)**2
sqrt(43481)
```
La matrice de distance euclidienne complète est donc :

```{r}
D_euc<-as.matrix(dist(tab[,2:3], upper = T))
kable(D_euc,caption = "Distance euclidienne", digits=0,row.names = T)
```

#### Classification k-means

L'application de la **méthode k-means** demande à l'utilisateur de fixer le nombre de classes souhaité. L'algorithme va ensuite tirer au sort des individus et procéder à des regroupements autour d'eux puis choisir la solution qui minimise les distance intra-classes et maximise les distances inter-classes. Si l'on opte pour pour 2 classes aboutira à la présence de deux groupes bien distincs : 

```{r}
km <-kmeans(D_euc,centers = 2,iter.max=20)
tab$km<-km$cluster
fviz_cluster(km, tab[, 2:3],stand = F,ellipse.type = "t") +
  coord_fixed(ratio = 1)+
  theme_light() +
  ggtitle("Méthode k-means / 2 classes ",subtitle = "Distance euclidienne non normée")

```



#### Classification ascendante hiérarchique


La **classification ascendante hiérarchique** utilise un algoritjme différent. Elle commence par regrouper ensemble les individus les plus proches selon un critère (ici : la distance moyenne entre individus) puis opère des fusions d'individus et de classe jusqu'à regrouper pous les individus en une seule classe. 

```{r}
cah<-hclust(dist(tab[,2:3]), method="average")
plot(cah,
     main = "CAH sur distance euclidienne",
     sub = "Critère de ressemblance moyenne",
     cex.main = 1.5,
     cex.sub = 1,
     cex.lab=1,
     xlab=NA,
     ylab="Distance moyenne",
     hang = -1)
```

Comme on peut le voir, elle regroupe en premier les individus n°2 et n°3 qui sont effectivement les plus proches ($D_{2,3} = 23$), puis les individus n°7 et n°8 ($D_{7,8} = 25$). A l'étape suivante, l'algorithme regroupe la classe $(2,3)$ avec l'individu n°4 pour former un groupe $(2,3,4)$ où la distance moyenne entre les trois individus est égale à 43. Elle regroupe ensuite les individus n°7 et n°9 dont la distance est de 79, etc. 

Sur le **dendrogramme (arbre hiérarchique de regroupement**, on peut repérer sur l'axe vertical la distance moyenne de regroupement des individus. On voit que le dernier regroupement entre les individus (1,2,3,4,5) et les individus (6,7,8,9) correspond à une distance moyenne proche de 300 ce qui souligne l'existence très nette de deux classes bien différentes.

### Distance euclidienne normée

Supposons maintenant que nous ayons décidé en début d'analyse de standardiser nos variables en leur donnant à chacune une moyenne de zéro et un écart-type de 1. Le tableau de départ serait alors celui-ci :

```{r}
don <- readRDS("fao/exemple1.RDS")
tab2<-don[,c(1,3,2)]
names(tab2)<-c("région","Lait_std","Alcool_std")
tab2$Alcool_std<-scale(tab2$Alcool_std)
tab2$Lait_std<-scale(tab2$Lait_std)
row.names(tab2)<-1:9
kable(tab2, caption = "Tableau standardisé",row.names = T, digits = c(0,2,2))
```

#### Distance

Du même coup, les distances entre les individus se trouvent modifiés ce que montre tout d'abord le graphique

```{r}
ggplot(tab2, aes(x=Lait_std,y=Alcool_std, label=1:9)) + 
       geom_point() +
       ggrepel::geom_label_repel()+
        coord_fixed(ratio = 1)+
  theme_light() +
  ggtitle("Espace à 2 dimensions normé") 


```

La matrice de distance euclidienne normée est désormais mesurée en nombre d'écart-type et on constate que les points se sont rapprochés dans le sens de la variable X (Lait) et éloignés dans le sens de la variable Y (Alcool).

```{r}
D_euc<-as.matrix(dist(tab2[,2:3], upper = T))
kable(D_euc,caption = "Distance euclidienne normée", digits=2,row.names = T)
```

#### Classification k-means

L'application de la **méthode k-means** donne des résultats proches de l'analyse précédente mais avec une opposition moins nette entre les deux groupes désormais 

```{r}
km <-kmeans(D_euc,centers = 2,iter.max=20)
tab$km<-km$cluster
fviz_cluster(km, tab2[, 2:3],stand = F,ellipse.type = "t") +
  coord_fixed(ratio = 1)+
  theme_light() +
  ggtitle("Méthode k-means / 2 classes ",subtitle = "Distance euclidienne non normée")

```



#### Classification ascendante hiérarchique


La **classification ascendante hiérarchique** conserve quant à elle la division en deux groupe mais on repère de légères modifications dans le bas de l'arbre. Ainsi, l'individu n°9 ne se regroupe plus en premier avec l'individu n°7 mais avec les individus n°6 et 8.

```{r}
cah<-hclust(dist(tab2[,2:3]), method="average")
plot(cah,
     main = "CAH sur distance euclidienne",
     sub = "Critère de ressemblance moyenne",
     cex.main = 1.5,
     cex.sub = 1,
     cex.lab=1,
     xlab=NA,
     ylab="Distance moyenne",
     hang = -1)
```

### Méthode de Ward

La troisième méthode, qui est en pratique la plus utilisée, consiste à opérer une classification non plus à l'aide des distances euclidiennes (normées ou non) mais à l'aide du **carré** de ces distances euclidiennes. Pourquoi ?

Parce que, comme nous l'avons vu dans le cadre du cours sur l'Analyse en Composantes Principales, la somme des distance euclidiennes au carré est proportionnelle à la somme des **variances** des différentes variables du tableau. La **méthode de Ward** va donc consiste à **minimiser la variance intra-classes** et **maximiser la variance inter-classes**.  Elle sera de ce fait très complémentaire avec l'ACP puisque cette dernière consiste précisément à concentrer la variance sur quelques axes significatifs.

Indiquons brièvement sans nous y attarder pour l'instant les résultats d'une ACP-CAH normée appliquée à notre tableau.

```{r}
acp<-PCA(tab[,2:3],graph = F)
cah<-HCPC(acp,nb.clust = 2)
```






## Bibliographie
